{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5f1696",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import mlflow\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da58eae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conf.\n",
    "EXPERIMENT_NAME = \"MNIST with CNN v.4\"\n",
    "TRACKING_URL = \"http://localhost:5500\"\n",
    "RUN_NAME = f\"run_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "RUN_DESCRIPTION = \"Runs training with the top 5 parameter combinations based on 1-epoch validation accuracy (dropout, batch size, learning rate, gamma, step, weight decay).\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fd046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed: int) -> None:\n",
    "    \"\"\"\n",
    "    Sets random seeds for reproducibility across random, numpy, and PyTorch.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b070f223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNeuralNet(nn.Module):\n",
    "    \"\"\"\n",
    "    A convolutional neural network for image classification.\n",
    "\n",
    "    Architecture:\n",
    "    - Two convolutional blocks with ReLU, MaxPool and Dropout.\n",
    "    - Fully connected classifier with two hidden layers and dropout.\n",
    "    - Outputs class logits (e.g. for CrossEntropyLoss).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dropout1=0.5, dropout2=0.25):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 32, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "            nn.Dropout(p=dropout2),\n",
    "        )\n",
    "\n",
    "        self.conv_block2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Dropout(p=dropout2),\n",
    "        )\n",
    "\n",
    "        self.classifier1 = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout1),\n",
    "            nn.Linear(256, 10),\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 7 * 7, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=dropout1),\n",
    "            nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    "        x = self.conv_block2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8130f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31927af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_and_log_model_checkpoint(model, path, artifact_dir):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(model.state_dict(), path)\n",
    "    mlflow.log_artifact(path, artifact_path=artifact_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f356393",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training_pipeline(params=None):\n",
    "    \"\"\"Log model architecture parameters.\"\"\"\n",
    "\n",
    "    default_params = {\n",
    "        \"epochs\": 100,\n",
    "        \"scheduler_step\": 1,\n",
    "        \"scheduler_gamma\": 0.9,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"weight_decay\": 0.00001,\n",
    "        \"batch_size\": 128,\n",
    "        \"num_workers\": 4,\n",
    "        \"dropout1\": 0.5,\n",
    "        \"dropout2\": 0.25,\n",
    "        \"patience\": 5,\n",
    "        \"min_save_accuracy\": 99.5,  # min validation accuracy to save the model\n",
    "    }\n",
    "\n",
    "    # update with custom parameters\n",
    "    if params is not None:\n",
    "        default_params.update(params)\n",
    "\n",
    "    # extract parameters\n",
    "    epochs = default_params[\"epochs\"]\n",
    "    scheduler_step = default_params[\"scheduler_step\"]\n",
    "    scheduler_gamma = default_params[\"scheduler_gamma\"]\n",
    "    learning_rate = default_params[\"learning_rate\"]\n",
    "    weight_decay = default_params[\"weight_decay\"]\n",
    "    batch_size = default_params[\"batch_size\"]\n",
    "    num_workers = default_params[\"num_workers\"]\n",
    "    dropout1 = default_params[\"dropout1\"]\n",
    "    dropout2 = default_params[\"dropout2\"]\n",
    "    patience = default_params[\"patience\"]\n",
    "    min_save_accuracy = default_params[\"min_save_accuracy\"]\n",
    "\n",
    "    # set up model, optimizer, scheduler and data transforms\n",
    "    device = select_device()\n",
    "    model = ConvNeuralNet().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer=optimizer, step_size=scheduler_step, gamma=scheduler_gamma)\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.RandomRotation(12), \n",
    "        transforms.RandomResizedCrop(28, scale=(0.9, 1.0)),\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),\n",
    "        transforms.ToTensor(),\n",
    "    ])\n",
    "\n",
    "    # load the MNIST dataset and prepare DataLoaders for training and validation\n",
    "    train_dataset = MNIST(root=\".\", train=True, transform=transform, download=True)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_dataset = MNIST(root=\".\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "    # set up and start the MLflow experiment run\n",
    "    mlflow.set_tracking_uri(TRACKING_URL)\n",
    "    mlflow.set_experiment(EXPERIMENT_NAME)\n",
    "\n",
    "    with mlflow.start_run(run_name=RUN_NAME):\n",
    "        run_id = mlflow.active_run().info.run_id\n",
    "        client = mlflow.tracking.MlflowClient()\n",
    "        client.set_tag(run_id, \"mlflow.note.content\", RUN_DESCRIPTION)\n",
    "\n",
    "        # log the parameters\n",
    "        all_params = {\n",
    "            \"model_summary\": str(model),\n",
    "            \"criterion\": \"CrossEntropyLoss\",\n",
    "            \"optimizer\": \"ADAM\",\n",
    "            \"scheduler_step\": scheduler_step,\n",
    "            \"scheduler_gamma\": scheduler_gamma,\n",
    "            \"learning_rate\": learning_rate,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"data_loader_workers\": num_workers,\n",
    "            \"dropout1\": dropout1,\n",
    "            \"dropout2\": dropout2,\n",
    "            \"patience\": patience,\n",
    "            \"min_save_accuracy\": min_save_accuracy,\n",
    "        }\n",
    "        mlflow.log_params(all_params)\n",
    "\n",
    "        # initialize early stopping counter and best validation loss\n",
    "        early_stop_counter, best_val_loss = 0, float(\"inf\")\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            total_train_loss, total_train_correct, total_train_samples = 0, 0, 0\n",
    "            total_val_loss, total_val_correct, total_val_samples = 0, 0, 0\n",
    "\n",
    "            model.train()\n",
    "            for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\"):\n",
    "                imgs, labels = imgs.to(device), labels.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(imgs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_train_loss += loss.item()\n",
    "                preds = outputs.argmax(1)\n",
    "                total_train_correct += (preds == labels).sum().item()\n",
    "                total_train_samples += labels.size(0)\n",
    "\n",
    "            avg_train_loss = total_train_loss / len(train_loader)\n",
    "            train_accuracy = 100 * total_train_correct / total_train_samples\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for imgs, labels in val_loader:\n",
    "                    imgs, labels = imgs.to(device), labels.to(device)\n",
    "                    outputs = model(imgs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    total_val_loss += loss.item()\n",
    "                    preds = outputs.argmax(1)\n",
    "                    total_val_correct += (preds == labels).sum().item()\n",
    "                    total_val_samples += labels.size(0)\n",
    "\n",
    "            avg_val_loss = total_val_loss / len(val_loader)\n",
    "            val_accuracy = 100 * total_val_correct / total_val_samples\n",
    "\n",
    "            print(f\"Epoch {epoch} | Train Loss: {avg_train_loss:.2f} | Train Acc: {train_accuracy:.2f}% | \"\n",
    "                f\"Val Loss: {avg_val_loss:.2f} | Val Acc: {val_accuracy:.2f}%\")\n",
    "\n",
    "            # log to MLflow\n",
    "            mlflow.log_metrics(\n",
    "                {\n",
    "                    \"loss/train\": avg_train_loss,\n",
    "                    \"loss/validate\": avg_val_loss,\n",
    "                    \"accuracy/train\": train_accuracy,\n",
    "                    \"accuracy/validate\": val_accuracy,\n",
    "                },\n",
    "                step=epoch,\n",
    "            )\n",
    "            scheduler.step()\n",
    "\n",
    "            improved = avg_val_loss < best_val_loss\n",
    "            should_save_checkpoint = (val_accuracy >= min_save_accuracy and epoch % 3 == 0)\n",
    "            is_final_epoch = epoch == epochs\n",
    "            should_early_stop = early_stop_counter >= patience\n",
    "\n",
    "            if improved:\n",
    "                best_val_loss = avg_val_loss\n",
    "                early_stop_counter = 0\n",
    "                best_model_path = (f\"ml_models/{EXPERIMENT_NAME}/{RUN_NAME}/best_model.pt\")\n",
    "                save_and_log_model_checkpoint(model, best_model_path, artifact_dir=\"best_model\")\n",
    "            else:\n",
    "                early_stop_counter += 1\n",
    "\n",
    "            if should_save_checkpoint:\n",
    "                checkpoint_path = (f\"ml_models/{EXPERIMENT_NAME}/{RUN_NAME}/epoch_{epoch}.pt\")\n",
    "                save_and_log_model_checkpoint(model, checkpoint_path, artifact_dir=\"checkpoints\")\n",
    "\n",
    "            if should_early_stop:\n",
    "                last_model_path = (f\"ml_models/{EXPERIMENT_NAME}/{RUN_NAME}/epoch_{epoch}.pt\")\n",
    "                save_and_log_model_checkpoint(model, last_model_path, artifact_dir=\"last_model\")\n",
    "                print(f\"Early stopping triggered after {epoch} epochs! No improvement for {patience} epochs.\")\n",
    "                break\n",
    "\n",
    "            if is_final_epoch:\n",
    "                last_model_path = (f\"ml_models/{EXPERIMENT_NAME}/{RUN_NAME}/epoch_{epoch}.pt\")\n",
    "                save_and_log_model_checkpoint(model, last_model_path, artifact_dir=\"last_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9dacd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "print(\"=\" * 50)\n",
    "print(EXPERIMENT_NAME.upper(), \"RUN NAME: \", RUN_NAME.upper())\n",
    "print(\"=\" * 50)\n",
    "seed_everything(42)\n",
    "\n",
    "# Top 5 configs from 1-epoch grid search\n",
    "top_param_grid = [\n",
    "    {\n",
    "        \"epochs\": 100,\n",
    "        \"dropout1\": 0.5,\n",
    "        \"dropout2\": 0.4,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_gamma\": 0.95,\n",
    "        \"scheduler_step\": 2,\n",
    "        \"weight_decay\": 0.0001,\n",
    "    },\n",
    "    {\n",
    "        \"epochs\": 100,\n",
    "        \"dropout1\": 0.5,\n",
    "        \"dropout2\": 0.4,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_gamma\": 0.8,\n",
    "        \"scheduler_step\": 2,\n",
    "        \"weight_decay\": 0.0001,\n",
    "    },\n",
    "    {\n",
    "        \"epochs\": 100,\n",
    "        \"dropout1\": 0.5,\n",
    "        \"dropout2\": 0.4,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_gamma\": 0.95,\n",
    "        \"scheduler_step\": 1,\n",
    "        \"weight_decay\": 1e-5,\n",
    "    },\n",
    "    {\n",
    "        \"epochs\": 100,\n",
    "        \"dropout1\": 0.5,\n",
    "        \"dropout2\": 0.4,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_gamma\": 0.8,\n",
    "        \"scheduler_step\": 1,\n",
    "        \"weight_decay\": 1e-5,\n",
    "    },\n",
    "    {\n",
    "        \"epochs\": 100,\n",
    "        \"dropout1\": 0.5,\n",
    "        \"dropout2\": 0.4,\n",
    "        \"batch_size\": 128,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"scheduler_gamma\": 0.95,\n",
    "        \"scheduler_step\": 2,\n",
    "        \"weight_decay\": 1e-5,\n",
    "    },\n",
    "]\n",
    "\n",
    "for i, params in enumerate(top_param_grid):\n",
    "    print(f\"\\n[RUN] Training top config {i+1}/{len(top_param_grid)}: {params}\")\n",
    "    run_training_pipeline(params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
